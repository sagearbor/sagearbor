name: My Independent Code Analyzer
on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  analyze-code:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout profile repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}

      - name: Install Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cloc gh python3-matplotlib python3-dateutil

      - name: Run Analysis
        env:
          GH_TOKEN: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          REPO_OWNER: ${{ github.repository_owner }}
        run: |
          cat << 'EOF' > analyzer.py
          import os, subprocess, json, base64
          import matplotlib.pyplot as plt
          from datetime import datetime

          OWNER = os.getenv("REPO_OWNER")
          
          # Base64 decode markers to bypass shell stripping
          START_MARKER = base64.b64decode("PCEtLSBMT0NfU1RBUlQgLS0+").decode('utf-8')
          END_MARKER = base64.b64decode("PCEtLSBMT0NfRU5EIC0tPg==").decode('utf-8')

          def run_cmd(cmd, cwd=None):
              try:
                  # Removed --quiet so we can capture output, but cloc might print to stderr
                  return subprocess.check_output(cmd, shell=True, cwd=cwd, text=True).strip()
              except subprocess.CalledProcessError as e:
                  print(f"Error running {cmd}: {e}")
                  return ""
              except Exception as e:
                  print(f"Unknown error: {e}")
                  return ""

          def main():
              if not os.path.exists("/tmp/repos"): os.makedirs("/tmp/repos")
              
              # 1. FETCH REPOS WITH DATES
              print("Fetching repository list...")
              cmd = f'gh repo list {OWNER} --source --json name,createdAt --limit 100'
              repos_json = run_cmd(cmd)
              
              if not repos_json:
                  print("No repositories found.")
                  return

              repos = json.loads(repos_json)
              results = []

              for r in repos:
                  name = r['name']
                  created_str = r['createdAt'] # Format: 2023-01-01T00:00:00Z
                  print(f"Analyzing {name}...")
                  
                  # Clone
                  repo_path = f"/tmp/repos/{name}"
                  run_cmd(f'gh repo clone {OWNER}/{name} -- --depth 1', cwd="/tmp/repos")
                  
                  if os.path.exists(repo_path):
                      # A. Count LOC using JSON output (More robust)
                      # We look for ANY code, excluding standard junk folders
                      cloc_cmd = f'cloc "{repo_path}" --json --exclude-dir=node_modules,vendor,dist,build,.github'
                      cloc_out = run_cmd(cloc_cmd)
                      
                      files, loc = 0, 0
                      try:
                          data = json.loads(cloc_out)
                          # SUM is the last entry in the JSON structure
                          if 'SUM' in data:
                              files = data['SUM']['nFiles']
                              loc = data['SUM']['code']
                      except Exception as e:
                          print(f"Warning: Could not parse cloc JSON for {name}. It might be empty. ({e})")

                      # B. Count Tests
                      # Look for test functions or markers in .py files
                      test_cmd = f'grep -rE "def test_|@pytest.mark" "{repo_path}" --include="*.py" | wc -l'
                      tests = run_cmd(test_cmd)
                      
                      # Parse Date for Sorting
                      try:
                          dt = datetime.strptime(created_str, "%Y-%m-%dT%H:%M:%SZ")
                          date_display = dt.strftime("%Y-%m-%d")
                      except:
                          dt = datetime.min
                          date_display = created_str

                      results.append({
                          "Repo": name, 
                          "Created": date_display,
                          "dt_obj": dt,
                          "Files": int(files), 
                          "Tests": int(tests) if tests.strip() else 0, 
                          "LOC": int(loc)
                      })
                      
                      run_cmd(f'rm -rf "{repo_path}"')

              if not results: return
              
              # 2. GENERATE GRAPH (Sorted by Creation Date: Oldest -> Newest)
              # Sort by datetime object for the graph
              results_by_date = sorted(results, key=lambda x: x['dt_obj'])
              
              # We plot all or top 15 to keep it readable
              graph_data = results_by_date[-15:] 
              
              names = [r['Repo'] for r in graph_data]
              locs = [r['LOC'] for r in graph_data]
              
              plt.figure(figsize=(10, 8)) # Taller figure
              # Create horizontal bar chart
              plt.barh(names, locs, color='#2ea44f')
              plt.xlabel('Lines of Code')
              plt.title('Repository Growth (Ordered by Creation Date)')
              plt.tight_layout()
              plt.savefig('stats.png')

              # 3. GENERATE TABLE (Sorted by Creation Date for User Preference)
              # Sort Newest First for the table
              results_table_order = sorted(results, key=lambda x: x['dt_obj'], reverse=True)
              
              table = "| Repo | Created | Files | Tests | LOC |\n|:---|:---|---:|---:|---:|\n"
              for res in results_table_order:
                  table += f"| {res['Repo']} | {res['Created']} | {res['Files']} | {res['Tests']} | {res['LOC']} |\n"

              # 4. UPDATE README
              new_section = f"{START_MARKER}\n### Lines of Code Visualized\n![Code Stats](stats.png)\n\n{table}\n{END_MARKER}"
              
              readme_content = ""
              if os.path.exists("README.md"):
                  with open("README.md", "r") as f:
                      readme_content = f.read()
              
              if START_MARKER in readme_content and END_MARKER in readme_content:
                  parts = readme_content.split(START_MARKER)
                  before = parts[0]
                  after = parts[1].split(END_MARKER)[1]
                  final_readme = before + new_section + after
              else:
                  final_readme = readme_content + "\n\n" + new_section

              with open("README.md", "w") as f:
                  f.write(final_readme)
              print("README updated successfully.")

          if __name__ == "__main__":
              main()
          EOF
          python3 analyzer.py

      - name: Commit and Push
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add README.md stats.png
          git commit -m "Update Code Stats" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.PERSONAL_ACCESS_TOKEN }}@github.com/${{ github.repository }}.git
